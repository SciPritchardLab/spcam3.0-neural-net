#include <misc.h>
#include <params.h>

module cloudbrain_module

!---------------------------------------------------------------------------------
! Purpose:
!
! Interface for Gentine-Pritchard machine learning scheme
! Let's rock the Casbah :)
! Author: Pierre Gentine and Mike Prithcard
!
!---------------------------------------------------------------------------------
  use shr_kind_mod,    only: r8 => shr_kind_r8
  use ppgrid,          only: pcols, pver, pverp
  use history,         only: outfld, addfld, add_default, phys_decomp
  use nt_FunctionsModule, only: nt_tansig
 use physconst,       only: gravit
  implicit none

  save

  private                         ! Make default type private to the module
! INSERT Pritch to Gentine: Please check these numbers. 
! Dimensions of neural net (must be consistent with numbers in define_neuralnet)
  integer, parameter :: hiddenlayerSize       = 20 ! Number of neurons (pritch to gentine: wasn't it 10?)
  integer, parameter :: outputlayerSize       = 24 
  integer, parameter :: nbhiddenlayers        = 1
!
! PUBLIC: interfaces
!
  public cloudbrain                 ! ML scheme
  public cloudbrainVerticalGrid
  integer, parameter :: inputlayerSize        = 30 ! Vertical dimension of training data. Pritch set to 30 to match pressure level vector sent by gentine
!
! Private data
!
contains

subroutine cloudbrain ( s,q,w,shf,lhf,ztodt,dsdt)
!----------------------------------------------------------------------- 
! 
! Purpose:
! Main driver for Gentine-Pritchard marchine learning convection scheme
! 
! Method: 
! performs deep convective adjustment based on deep learning !
! Author:Pierre Gentine and Mike Pritchard.
!
! This is contributed code not fully standardized by the CAM core group.
! All variables have been typed, where most are identified in comments
! The current procedure will be reimplemented in a subsequent version
! of the CAM where it will include a more straightforward formulation
! and will make use of the standard CAM nomenclature
! 
!-----------------------------------------------------------------------

! model state profiles pre-interpolated to NN-fixed-grid:
real (r8), intent(in) :: s(inputlayerSize) ! dry static energy
real (r8), intent(in) :: q(inputlayerSize) ! q
real (r8), intent(in) :: w(inputlayerSize) ! w (m/s)
real (r8), intent(in) :: shf,lhf ! model shf,lhf
real(r8), intent(in) :: ztodt         ! 2 delta t (model time increment)
real (r8), intent(inout) :: dsdt(inputlayerSize) ! NN-predicted tendency of DSE
! (will be interpolated downstream back to GCM vertical grid)

! Essence of neural net (will be defined in define_neuralnet)
real(r8) :: mu_in(inputlayerSize) ! normalization input mean
real(r8) :: std_in(inputlayerSize)  ! normalization input standard deviation
real(r8) :: bias_input(hiddenlayerSize)
real(r8) :: bias_output(hiddenlayerSize)
real(r8) :: weights_input(hiddenlayerSize,inputlayerSize) 
real(r8) :: weights_output(outputlayerSize,hiddenlayerSize) 

!-------

real(r8) :: input(inputlayerSize) ! master array to contain all input fields from GCM
real(r8) :: output(outputlayerSize) ! results from neural net given the input above. 
real(r8) :: h(hiddenlayerSize) 

integer :: i,j

call define_neuralnet (mu_in, std_in, weights_input, bias_input, bias_output,weights_output)

    input(1:inputlayerSize)  = s(1:inputlayerSize)
    input(inputlayerSize+1:2*inputlayerSize)    = q(1:inputlayerSize)
    input(2*inputlayerSize+1:3*inputlayerSize)  = w(1:inputlayerSize)
    input(3*inputlayerSize+1)    = shf
    input(3*inputlayerSize+2)    = lhf

    ! Mike: this example is for wdse flux
    ! Normalize data
    input(:)     =  (input(:)-mu_in(:))/std_in(:)

    ! input layer weights+bias and then take tansig of result
    h(:) = 0.
    do i = 1,hiddenlayerSize
        h(i) = h(i) + bias_input(i)
        do j = 1,inputlayerSize
            h(i) = h(i) + weights_input(i,j)*input(j)
        end do
    h(i) = nt_tansig(h(i))
    end do

    ! only one first hidden layer for now
    !do k = 1,nbhiddenlayers-1
    !    h(:) = 0.
    !    do i = 1,hiddenlayerSize
    !        do j = 1,inputlayerSize
    !            h(i) = nt_tansig(bias(i,j) + weights(k,i,j)*input(j))
    !        end do
    !    end do
    !end do


    ! output layer
    output(:) = 0.
    do i = 1,outputlayerSize
        output(i) = output(i) + bias_output(i)
        do j = 1,hiddenlayerSize
            output(i) = output(i) + weights_output(i,j)*h(j)
        end do
    end do
    ! now we have the profile of wdse flux ;-) (24 sigma-levels for now)
    do i=1,pver
        dsdt(i) = 0.
    end do
    ! Pritch to gentine, where is dpp defined? What is the vertical grid of the output
    ! space? Based on dim sizes, it appears to be a different grid than the
    ! input.
    do i = 1,outputlayerSize-1
         ! Pritch commented this out becuase dpp undefined:
!        dsdt(i) = gravit*(2.*delt)/dpp(kk,i)*(output(i) - output(i+1)) ! tendency: TO MIKE: I don't know why they use 2*delta t
!thorughout for the time step instead of delta t... also I am not sure
!whther we should use gravit or rgrav for the conversion from z to Pa.
    end do





! END GENTINE ADDITIONS




   return
end subroutine cloudbrain


subroutine define_neuralnet (mu_in, std_in, weights_input, bias_input,bias_output, weights_output) 
real(r8), intent(out) :: mu_in(inputlayerSize)
real(r8), intent(out) :: std_in(inputlayerSize)
real(r8), intent(out) :: bias_input(hiddenlayerSize)
real(r8), intent(out) :: bias_output(hiddenlayerSize)
real(r8), intent(out) :: weights_input(hiddenlayerSize,inputlayerSize)
real(r8), intent(out) :: weights_output(outputlayerSize,hiddenlayerSize)
! define weights
! Pritch to Gentine. HEY this is giving an array dimensions non conform error on compile:
mu_in(1:inputlayerSize)    = (/ 291.097694027879,291.083241439577,291.141007167882,291.360706190071,291.760041057174,292.320270587951,292.987216516810,294.213708859079,&
295.921729404634        ,297.990198114420       ,300.206797853928       ,302.306564225387       ,304.005066172499       ,305.491215760534       ,307.114620386763       ,&
309.216916587025        ,311.921906662719       ,315.301778731259       ,319.875250402276       ,325.575856555259       ,0.0113315689525928     ,0.0111171245505797     ,&
0.0106612990006062      ,0.00996085681441109    ,0.00913471614608694    ,0.00827832873639303    ,0.00745915639052255    ,0.00619246323612752    ,&
0.00471833056189742,    0.00326813865376010,    0.00202377000021843,    0.00111553145128881,    0.000595393457737442,   0.000313040002006912,&
0.000164477418391419,   8.73882506712451e-05,   4.71349132171886e-05,   2.50386748332238e-05,   1.07761180239012e-05,   5.43763226186672e-06,&
0.000258780060325041,   0.000240796747691326,   0.000237823480099460,   0.000240104097621787,   0.000228519445233746,   0.000205917312985066,&
0.000194982945502024,   0.000205542243344631,   0.000191417915469026,   0.000141167517171237,   0.000104108109227069,   0.000127672996175335,&
0.000165562279573950,   0.000150790639516362,   0.000145234739633937,   0.000118407039748620,   8.99255052260084e-05,   7.96865348373704e-05,&
6.71857632000372e-05,   6.23131779174247e-05,   0.917819779190129,      4342.68272582789/)

! Pritch to Gentine. HEY same issue here as above.
std_in(1:inputlayerSize) = (/ 9.24417062011163, 9.19974670087717,       9.19874872947645,       9.25738527208696,       9.37699563534203,       9.55109860905078,       9.75941222474071,&
10.1617111002672,       10.7660230884776,       11.6217748876701,       12.7880303769980,       14.0842626088679,       14.9460220142231,       15.0818583208846,       &
14.2449786067721,       12.4573667330226,       10.2214348323943,       8.02758267175890,       5.83389699703865,       4.18338556131189,       0.00536972567438357,&
0.00529583036338539,    0.00510804841322640,    0.00480719100349291,    0.00446529145904821,    0.00412505521244398,    0.00381516461644811,&
0.00333666549263138,    0.00276542290743095,    0.00214721366405132,    0.00150832889342326,    0.000955832401340574,   0.000583074026795674,&
0.000341313091929598,   0.000195697346546233,   0.000114597051051331,   7.10555667678295e-05,   4.45568044867781e-05,   2.39790863319211e-05,&
1.21354944890208e-05,   0.0120726069018580,     0.0166150646364670,     0.0187194067656509,     0.0202098006251255,     0.0218775122273536,     0.0237156011779202,&
0.0255769089722447,     0.0283671490110540,     0.0313437223704286,     0.0339636899685782,     0.0369372475934862,     0.0411850116533137,     0.0448591046637976,&
0.0468387448156213,     0.0470410966974761,     0.0439597934648174,     0.0361810869641972,     0.0248676289071130,     0.0155903178840457,     0.00935953314265917,&
1.76915024206503,       6574.89100033530/)


bias_input(:) = (/ 0.17, -0.26, -1.29,  0.30, -0.18,  0.17, -0.24, -0.34,  0.14, -0.28,  0.22,  0.29,  0.13, -0.27, -1.52,  0.19, -0.21, -0.54, -0.65,  0.29/)
bias_output(:) = (/ -0.17, -0.32,  0.39,  0.63,  0.68,  2.14,  3.16,  3.02,  2.99, -3.29, -0.36, -0.46, -4.17, -2.26,  3.52,  3.86,  0.29, -0.52,  0.15,  1.51/)
weights_input(:,:) = reshape((/  0.11, -0.10, -0.03,  1.00, -0.11,  0.07, -0.12, -0.08,  0.13, -0.92,  0.12,  0.10,  0.03, -0.11, -0.05, -0.01, -0.11, -0.32, -0.05,  0.13, -0.14,  0.13,  0.04, -0.24,  0.15, -0.01,  0.14,  0.10, -0.18,  0.18, -0.14, -0.12, -0.07,  0.13, -0.36, -0.18,  0.14,  0.10,  0.07, -0.16, -0.01,  0.01,  0.01, -0.14,  0.01, -0.20,  0.02,  0.02,  0.04,  0.17, -0.04, -0.03, -0.06,  0.03,  0.44,  0.04,  0.02, -0.04,&
  0.00, -0.05, -0.04,  0.02,  0.00, -0.89,  0.03,  0.14,  0.03,  0.03, -0.11,  0.84, -0.00, -0.00, -0.05,  0.01,  0.10,  0.19,  0.02,  0.43,  0.02,  0.03,  0.02, -0.02, -0.00, -0.62, -0.03, -0.08, -0.03, -0.03,  0.07,  0.57, -0.00,  0.01,  0.20, -0.00,  0.17,  0.07, -0.02,  0.19, -0.02, -0.02,  0.09, -0.08, -0.03, -0.23, -0.10,  0.14, -0.09, -0.07,  0.07,  0.14,  0.11,  0.08,  0.07, -0.09, -0.28, -0.05, -0.10, -0.09, -0.04,  0.09,  0.05, -0.04, -0.01, -0.32, -0.05,  0.04, -0.05, -0.04,  0.06,  0.33,  0.05,  0.05,  0.05, -0.05,  0.08,  0.20, -0.05,  0.17, -0.02,  0.08, -0.14,  0.12,  0.04,  0.66,  0.14, -0.14,  0.14,  0.11, -0.14, -0.57, -0.14, -0.13, -0.16,  0.13,  0.24, -0.17,  0.14, -0.12,  0.07, -0.16, -0.09,  0.08,  0.03,  1.21,  0.10, -0.09,  0.09,  0.07, -0.08, -1.14, -0.09, -0.09, -0.26,  0.08, -0.45, -0.26,  0.10, -0.46,  0.04, -0.09,  0.19, -0.17, -0.06, -0.57, -0.20,  0.20, -0.21, -0.16,  0.19,  0.54,  0.20,  0.18,  0.37, -0.18,  0.14,  0.24, -0.20,  0.24, -0.09,&
  0.21, -0.13,  0.11,  0.04, -0.54,  0.13, -0.14,  0.14,  0.11, -0.14,  0.48, -0.14, -0.12, -0.23,  0.13,  0.12, -0.03,  0.13,  0.11,  0.06, -0.14,  0.05, -0.04, -0.02,  0.38, -0.05,  0.06, -0.05, -0.04,  0.05, -0.34,  0.05,  0.04,  0.11, -0.05, -0.02, -0.01, -0.05, -0.08, -0.02,  0.05, -0.02,  0.02,  0.01,  0.67,  0.02, -0.02,  0.02,  0.02, -0.01, -0.63, -0.01, -0.02, -0.12,  0.01, -0.24, -0.13,  0.02, -0.25,  0.01, -0.02, -0.03,  0.03,  0.01,  0.34,  0.03, -0.03,  0.03,  0.02, -0.03, -0.29, -0.03, -0.03, -0.04,  0.03,  0.07, -0.05,  0.03, -0.08,  0.01, -0.03,  0.00, -0.00, -0.00, -0.16, -0.00,  0.00, -0.00, -0.00,  0.00,  0.15,  0.00,  0.00,  0.02, -0.00, -0.00,  0.02, -0.00,  0.05, -0.00,  0.00, -0.00,  0.00,  0.00,  0.13,  0.01, -0.00,  0.00,  0.00, -0.00, -0.12, -0.00, -0.00, -0.02,  0.00, -0.00, -0.02,  0.01, -0.04,  0.00, -0.00, -0.01,  0.01,  0.00,  0.15,  0.01, -0.01,  0.01,  0.01, -0.01, -0.14, -0.01, -0.01, -0.02,  0.01, -0.04, -0.03,  0.01, -0.04,  0.00, -0.01,  0.01, -0.01, -0.00, -0.56, -0.01,  0.01, -0.01, -0.01,  0.01,  0.51,  0.01,  0.01,  0.05, -0.01,  0.09,  0.08, -0.02,  0.16, -0.01,  0.01, -0.03,  0.02,  0.01,  0.59,  0.03, -0.02,  0.03,  0.02, -0.02, -0.54, -0.02, -0.03, -0.07,  0.02, -0.08, -0.09,  0.03, -0.16,  0.01, -0.03,  0.01, -0.01, -0.00, -0.17, -0.01,  0.01, -0.01, -0.01,  0.01,  0.15,  0.01,  0.01,  0.02, -0.01,  0.03,  0.03, -0.01,  0.05,&
 -0.00,  0.01,  0.02, -0.02, -0.01, -1.12, -0.03,  0.03, -0.02, -0.02,  0.02,  1.02,  0.02,  0.03,  0.11, -0.02,  0.16,  0.15, -0.03,  0.34, -0.01,  0.02, -0.04,  0.04,  0.01, -0.08,  0.04, -0.02,  0.04,  0.03, -0.05,  0.03, -0.04, -0.04, -0.05,  0.04, -0.14, -0.08,  0.04, -0.01,  0.02, -0.05,  0.04, -0.03, -0.01,  0.21, -0.04,  0.00, -0.04, -0.03,  0.05, -0.14,  0.03,  0.03,  0.04, -0.03,  0.10,  0.06, -0.04, -0.05, -0.02,  0.04, -0.04,  0.03,  0.01,  0.13,  0.04, -0.00,  0.04,  0.03, -0.05, -0.11, -0.03, -0.03, -0.08,  0.03,  0.04, -0.02,  0.04, -0.00,  0.02, -0.02,  0.02, -0.02, -0.00,  0.09, -0.02, -0.00, -0.02, -0.02,  0.03, -0.10,  0.01,  0.01,  0.05, -0.01, -0.06, -0.01, -0.02, -0.02, -0.01,  0.01,  0.01, -0.01, -0.00, -0.11, -0.01,  0.03, -0.01, -0.01,  0.01,  0.08,  0.02,  0.01,  0.00, -0.02, -0.07, -0.02, -0.02, -0.02, -0.01,  0.02, -0.02,  0.02,  0.01, -0.08,  0.02, -0.03,  0.02,  0.02, -0.02,  0.09, -0.02, -0.02, -0.02,  0.02,  0.10,  0.03,  0.02,  0.05,  0.01, -0.02,  0.01, -0.01, -0.00,  0.10, -0.01,  0.01, -0.01, -0.01,  0.01, -0.09,  0.01,  0.01,  0.02, -0.01, -0.03, -0.01, -0.01, -0.02, -0.00,  0.01, -0.02,  0.02,  0.01,  0.10,  0.02, -0.02,  0.02,  0.02, -0.02, -0.09, -0.02, -0.02, -0.04,  0.02,  0.00, -0.03,  0.02, -0.04,  0.01, -0.02,  0.02, -0.02, -0.01,  0.01, -0.03,  0.03, -0.03, -0.02,  0.02, -0.01,  0.03,  0.02,  0.04, -0.02, -0.02,  0.02, -0.02,&
  0.00, -0.01,  0.03, -0.02,  0.02,  0.01, -0.13,  0.02, -0.02,  0.02,  0.01, -0.02,  0.12, -0.02, -0.02, -0.02,  0.02,  0.03,  0.00,  0.02,  0.03,  0.01, -0.02,  0.01, -0.01, -0.00,  0.14, -0.01,  0.01, -0.01, -0.01,  0.01, -0.13,  0.01,  0.01,  0.01, -0.01, -0.03, -0.01, -0.01, -0.04, -0.01,  0.01, -0.01,  0.01,  0.00, -0.04,  0.01, -0.01,  0.01,  0.01, -0.01,  0.04, -0.01, -0.01, -0.01,  0.01,  0.01, -0.00,  0.01,  0.01,  0.00, -0.01, -0.00,  0.00,  0.00,  0.09,  0.00, -0.00,  0.00,  0.00,  0.00, -0.08, -0.00, -0.00, -0.00,  0.00, -0.01, -0.01,  0.00, -0.02,  0.00, -0.00, -0.00,  0.00,  0.00, -0.08,  0.00, -0.00,  0.00,  0.00, -0.00,  0.07, -0.00, -0.00,  0.00,  0.00,  0.01,  0.01,  0.00,  0.02,  0.00, -0.00,  0.00, -0.00, -0.00,  0.10, -0.00,  0.00, -0.00, -0.00,  0.00, -0.09,  0.00,  0.00, -0.00, -0.00, -0.01, -0.01, -0.00, -0.02, -0.00,  0.00,  0.00, -0.00, -0.00, -0.09, -0.00,  0.00, -0.00, -0.00,  0.00,  0.09,  0.00,  0.00,  0.01, -0.00,  0.01,  0.01, -0.00,  0.02, -0.00,  0.00, -0.01,  0.01,  0.00,  0.09,  0.01, -0.01,  0.01,  0.01, -0.01, -0.08, -0.01, -0.01, -0.02,  0.01, -0.00, -0.02,  0.01, -0.02,  0.00, -0.01,  0.01, -0.01, -0.00,  0.00, -0.01,  0.01, -0.01, -0.00,  0.01, -0.00,  0.01,  0.01,  0.01, -0.01, -0.00,  0.00, -0.01, -0.00, -0.00,  0.01, -0.00,  0.00,  0.00, -0.03,  0.00, -0.00,  0.00,  0.00, -0.00,  0.02, -0.00, -0.00, -0.00,  0.00,  0.00, -0.00,  0.00,&
  0.01,  0.00, -0.00,  0.01, -0.02, -0.01,  0.39, -0.02, -0.00, -0.02, -0.01,  0.02, -0.16,  0.01,  0.02,  0.06, -0.01,  0.12,  0.16, -0.02,  0.03, -0.01,  0.02, -0.05,  0.04,  0.01, -0.62,  0.05, -0.04,  0.05,  0.03, -0.04,  0.43, -0.05, -0.05, -0.09,  0.04, -0.18, -0.21,  0.05,  0.01,  0.02, -0.06,  0.05, -0.05, -0.01,  0.43, -0.06,  0.06, -0.06, -0.04,  0.07, -0.43,  0.07,  0.05, -0.02, -0.06, -0.03, -0.04, -0.05, -0.16, -0.03,  0.08, -0.05,  0.05,  0.01, -0.24,  0.05, -0.03,  0.05,  0.04, -0.06,  0.29, -0.06, -0.04, -0.05,  0.05,  0.30,  0.13,  0.05,  0.23,  0.02, -0.04,  0.04, -0.02, -0.01,  0.19, -0.03,  0.06, -0.03, -0.03,  0.03, -0.17,  0.03,  0.02,  0.11, -0.02, -0.26, -0.03, -0.03, -0.04, -0.01,  0.01,  0.01,  0.01,  0.00, -0.14,  0.00,  0.01,  0.01,  0.00, -0.01,  0.11, -0.01, -0.01,  0.03,  0.01, -0.08, -0.06,  0.01, -0.07,  0.00, -0.02,  0.00, -0.00, -0.00,  0.08,  0.01, -0.04,  0.01,  0.02, -0.01, -0.04,  0.01,  0.01, -0.08, -0.00,  0.27,  0.06, -0.00, -0.06,  0.01,  0.03,  0.00, -0.01, -0.00,  0.01, -0.00, -0.03, -0.00, -0.00, -0.02, -0.03,  0.00,  0.00,  0.07, -0.00, -0.19, -0.05, -0.01,  0.11, -0.00, -0.01, -0.01,  0.01,  0.01, -0.02,  0.01, -0.01, -0.01, -0.01,  0.01,  0.02, -0.03, -0.01, -0.03,  0.01,  0.10,  0.02,  0.01, -0.05, -0.00,  0.00,  0.00, -0.01, -0.01,  0.00, -0.00,  0.05,  0.00,  0.01, -0.02, -0.00,  0.03, -0.00, -0.06, -0.01, -0.06,  0.01,&
 -0.01,  0.01,  0.00, -0.01, -0.01,  0.01,  0.00,  0.01,  0.02, -0.05,  0.01, -0.01,  0.01, -0.01,  0.00,  0.00,  0.10,  0.00,  0.03,  0.00,  0.01, -0.01, -0.01, -0.00,  0.01, -0.00,  0.00, -0.02, -0.02,  0.00, -0.01,  0.02,  0.04,  0.01,  0.01, -0.01, -0.09, -0.01, -0.03, -0.00, -0.02,  0.02,  0.01,  0.00, -0.02,  0.01, -0.01,  0.04,  0.02,  0.04,  0.02, -0.02, -0.08, -0.03, -0.03,  0.01,  0.09,  0.00,  0.02, -0.02,  0.01, -0.03, -0.01,  0.00,  0.03, -0.01, -0.00, -0.04, -0.02, -0.08, -0.01,  0.01,  0.09,  0.03,  0.01, -0.02, -0.10,  0.01, -0.03,  0.03, -0.00,  0.01, -0.00,  0.01, -0.04,  0.02,  0.00,  0.02, -0.00,  0.09,  0.00, -0.01, -0.06, -0.02,  0.02,  0.01,  0.14,  0.01,  0.07, -0.05,  0.00, -0.00,  0.01,  0.00,  0.04, -0.02,  0.01,  0.01, -0.00, -0.06,  0.01, -0.00,  0.02, -0.01, -0.01, -0.00, -0.16, -0.00, -0.14,  0.06, -0.01,  0.00, -0.01, -0.01, -0.01, -0.00, -0.01, -0.07,  0.02,  0.06, -0.01,  0.00,  0.02,  0.05,  0.00, -0.00,  0.14, -0.01,  0.24, -0.07, -0.00,  0.03,  0.01, -0.00, -0.00,  0.01,  0.00,  0.09,  0.00, -0.05,  0.01,  0.00, -0.02, -0.07,  0.00,  0.00, -0.11, -0.00, -0.25,  0.04,  0.01, -0.05,  0.00,  0.01,  0.01, -0.01, -0.00, -0.07, -0.02, -0.00, -0.00, -0.01, -0.00,  0.04, -0.00, -0.00,  0.04, -0.00,  0.17, -0.02, -0.00,  0.04, -0.00, -0.02, -0.01,  0.00,  0.00,  0.04,  0.00,  0.02,  0.00,  0.01, -0.00, -0.02, -0.00, -0.00,  0.01, -0.00, -0.06,  0.00,  0.00, -0.02,  0.00,  0.01,  0.00, -0.00, -0.00, -0.04, -0.00,  0.00, -0.00, -0.00,  0.00,  0.03,  0.00,  0.00,  0.01, -0.00,  0.00,  0.01, -0.00,  0.01, -0.00,  0.00,  0.00, -0.00, -0.00, -0.03, -0.00,  0.00, -0.00, -0.00,  0.00,  0.03,  0.00,  0.00,  0.00, -0.00,  0.00,  0.00, -0.00,  0.01, -0.00,  0.00/), (/hiddenlayerSize,inputlayerSize/))

 weights_output(:,:) = reshape((/  0.03,  0.37,  0.89,  1.47,  1.79,  1.72,  1.36,  0.72,  0.28, -0.13, -0.10,  0.07,  0.09,  0.14, -0.00,  0.09,  0.05,  0.04, -0.00, -0.09,  0.79,  1.18,  1.69,  1.96,  1.74,  1.28,  0.76,  0.54,  1.23,  0.87,  1.37,  1.10,  0.04, -0.43, -0.69, -1.23, -0.84, -0.16, -0.15, -0.36,  1.12,  1.16,  1.50,  1.24,  1.02,  2.41,  3.43,  4.08,  4.67, -2.34,  0.92, -0.67, -4.67, -2.64,  3.89,  4.69,  0.01, -0.57,  0.67,  2.70,  1.27,  0.86,  0.55,  0.42,  0.40,  0.36,  0.29,  0.24,  0.19,  0.14,  0.09,  0.06,  0.05,  0.04,  0.04,  0.03,  0.03,  0.03,  0.04,  0.04, -0.09, -0.00,  0.13,  0.21,  0.24,  0.32,  0.48,  0.75,  0.96,  0.84,  0.55, -0.05, -0.01, -0.11, -0.16,  0.21,  0.35, -0.12, -0.47, -0.32,  0.00,  0.11,  0.23,  0.31,  0.25,  0.05, -0.18, -0.32, -0.07,  0.08, -0.16, -0.05,  0.00, -0.04,  0.01,  0.01, -0.01, -0.08, -0.05,  0.04, -0.13, -0.10, -0.04, -0.10, -0.25, -0.35, -0.52, -1.35, -1.91, -0.19,  0.88,  1.16,  1.11,  0.59,  0.52,  0.66,  0.24,  0.08,  0.06,  0.02, -0.23,  0.07,  0.32,  0.38,  0.41,  0.65,  0.98,  0.50, -0.58, -0.10, -0.48, -0.62, -1.12, -1.17, -1.58, -1.31, -1.03, -0.82, -0.27,  0.42, -0.03, -0.06, -0.11, -0.15, -0.24, -0.45, -0.68, -0.75, -0.50, -0.30,  0.15,  0.13, -0.03,  0.08, -0.06,  0.02,  0.10, -0.08, -0.18, -0.18,  1.46,  1.02,  0.61,  0.43,  0.45,  0.47,  0.37,  0.24,  0.24,  0.17,  0.11,  0.07,  0.06,  0.05,  0.05,  0.04,  0.03,  0.04,  0.04,  0.05,  0.61,  0.68,  0.62,  0.44,  0.30,  0.35,&
  0.46,  0.23,  0.09,  1.03,  0.75, -0.36, -0.53, -0.02,  0.22, -0.01, -0.07,  0.03,  0.02, -0.06,  0.67,  0.60,  0.58,  0.34, -0.01, -0.09, -0.21, -1.16, -2.38, -2.44, -1.38, -1.17, -1.14, -1.47, -0.87, -0.66, -0.58, -0.49, -0.85, -0.97, -0.33, -0.35, -0.32, -0.19, -0.03,  0.03,  0.02,  0.02, -0.10, -0.09,  0.08,  0.01, -0.00,  0.01, -0.00, -0.03,  0.01, -0.03,  0.02,  0.09,  0.15, -0.00, -0.15, -0.18, -0.05,  0.15,  0.27,  0.17, -0.22, -0.57, -0.84, -1.04, -0.14,  0.69,  0.59, -0.24, -1.03, -1.10, -0.95, -0.82,  0.10,  0.13,  0.37,  0.26, -0.02,  0.15,  0.36, -0.05,  0.17,  0.06,  0.07,  0.02,  0.03,  0.01,  0.03,  0.02,  0.08, -0.05,  0.13, -0.06, -0.91, -0.94, -0.69, -0.43, -0.33, -0.29, -0.29, -0.35, -0.10, -0.01, -0.04, -0.07, -0.07, -0.05, -0.04, -0.01, -0.02,  0.02, -0.02,  0.01,  0.68,  0.96,  1.16,  1.04,  0.57,  0.02, -0.49, -1.03, -1.22, -1.22, -1.03, -1.59, -0.74, -0.24, -0.14,  0.20,  0.70,  0.87,  0.64,  0.51,  0.21,  0.27,  0.37,  0.37,  0.13, -0.12, -0.08,  0.26, -0.00,  0.01,  0.00,  0.02, -0.01, -0.01, -0.02,  0.00,  0.02,  0.01,  0.02, -0.02, -3.08, -3.56, -3.24, -2.22, -1.17, -0.62, -0.56, -1.12, -2.05, -2.71, -1.97,  0.83,  0.50,  0.93,  2.05,  1.59,  2.21,  1.46, -0.10, -0.79,  0.18,  0.48,  0.75,  0.59,  0.24,  0.31,  0.62,  0.67,  0.41,  0.06,  0.35,  0.53,  0.78,  0.84,  0.59,  0.28,  0.10,  0.07,  0.02,  0.38/), (/outputlayerSize,hiddenlayerSize/))

end subroutine define_neuralnet

subroutine cloudbrainVerticalGrid(pnet)
real(r8), intent(out) :: pnet(inputlayerSize)

pnet = (/ 3.64346569404006, 7.59481964632869, 14.3566322512925,24.6122200042009, 38.2682997733355, 54.5954797416925, 72.0124505460262,87.8212302923203, 103.317126631737, 121.547240763903, 142.994038760662,168.225079774857, 197.908086702228, 232.828618958592, 273.910816758871,   322.241902351379, 379.100903868675, 445.992574095726, 524.687174707651,  609.778694808483, 691.389430314302, 763.404481112957, 820.858368650079,    859.53476652503, 887.020248919725, 912.644546944648, 936.198398470879,    957.485479535535, 976.325407391414, 992.556095123291 /)

end subroutine cloudbrainVerticalGrid

end module cloudbrain_module
